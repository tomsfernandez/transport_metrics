{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "## Indicadores de tiempo en parada utilizando información de la API pública de Data BA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting package metadata: done\n",
            "Solving environment: / \n",
            "The environment is inconsistent, please check the package plan carefully\n",
            "The following packages are causing the inconsistency:\n",
            "\n",
            "  - conda-forge/linux-64::matplotlib\u003d\u003d3.0.3\u003dpy37_1\n",
            "done\n",
            "\n",
            "\n",
            "\u003d\u003d\u003e WARNING: A newer version of conda exists. \u003c\u003d\u003d\n",
            "  current version: 4.6.14\n",
            "  latest version: 4.7.12\n",
            "\n",
            "Please update conda by running\n",
            "\n",
            "    $ conda update -n base conda\n",
            "\n",
            "\n",
            "\n",
            "# All requested packages already installed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!sudo conda install -c conda-forge -y pyarrow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pygeohash in /opt/conda/lib/python3.7/site-packages (1.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pygeohash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\u0027PYSPARK_SUBMIT_ARGS\u0027] \u003d \u0027--driver-class-path\u003d/home/jovyan/work/lib/postgresql-42.2.6.jar pyspark-shell\u0027"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "import pandas as pd \n",
        "import numpy\n",
        "import matplotlib.pyplot as plt \n",
        "import pygeohash as pgh\n",
        "from pyspark.sql import SparkSession, Row\n",
        "from pyspark.sql.window import Window\n",
        "import pyspark.sql.functions as F \n",
        "import pyspark\n",
        "from pyspark.sql.functions import lit, udf, col, unix_timestamp, udf, pandas_udf, PandasUDFType\n",
        "from pyspark.sql.types import DoubleType, IntegerType"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "spark \u003d SparkSession.builder.appName(\"BaPoints\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[route_id: string, trip_id: string, last_stop_id: string, timestamp: bigint, vehicle_id: string, vehicle_label: string, start_time: string, start_date: string, latitude: double, longitude: double, speed: double]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "points \u003d spark.read.json(\u0027../Datasets/ba_points/points_50000_new.json\u0027)\n",
        "points \u003d points.drop(\u0027_id\u0027).selectExpr(\u0027_vehicle._trip._route_id as route_id\u0027,\n",
        "                              \u0027_vehicle._trip._trip_id as trip_id\u0027,\n",
        "                              \u0027_vehicle._stop_id as last_stop_id\u0027,\n",
        "                              \u0027_vehicle._timestamp as timestamp\u0027,\n",
        "                              \u0027_vehicle._vehicle._id as vehicle_id\u0027,\n",
        "                              \u0027_vehicle._vehicle._label as vehicle_label\u0027,\n",
        "                              \u0027_vehicle._trip._start_time as start_time\u0027,\n",
        "                              \u0027_vehicle._trip._start_date as start_date\u0027,         \n",
        "                              \u0027_vehicle._position._latitude as latitude\u0027,\n",
        "                              \u0027_vehicle._position._longitude as longitude\u0027,\n",
        "                              \u0027_vehicle._position._speed as speed\u0027)\n",
        "points \u003d points.where(~F.isnull(points.start_time))\n",
        "points.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
        "points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, count: bigint]"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "point_count_by_vehicle \u003d points.groupBy(\u0027vehicle_id\u0027).count()\n",
        "point_count_by_vehicle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[route_id: int, agency_id: int, route_short_name: string, route_long_name: string, route_desc: string, route_type: int]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "routes \u003d spark.read.csv(\u0027../Datasets/ba_points/routes.txt\u0027, inferSchema\u003dTrue, header\u003dTrue)\n",
        "routes \u003d routes.drop(\u0027route_url\u0027,\u0027route_color\u0027, \u0027route_text_color\u0027)\n",
        "routes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[trip_id: string, stop_id: bigint, stop_sequence: int, timepoint: int]"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stop_trips \u003d spark.read.csv(\u0027../Datasets/ba_points/stop_times.txt\u0027, inferSchema\u003dTrue, header\u003dTrue)\n",
        "stop_trips \u003d stop_trips.drop(\u0027arrival_time\u0027, \u0027departure_time\u0027, \u0027stop_headsign\u0027,\n",
        "                             \u0027pickup_type\u0027, \u0027drop_off_type\u0027, \u0027shape_dist_traveled\u0027)\n",
        "stop_trips.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
        "stop_trips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[route_id: int, service_id: int, trip_id: string, trip_headsign: string, trip_short_name: string, direction_id: int, exceptional: int]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "trips \u003d spark.read.csv(\u0027../Datasets/ba_points/trips.txt\u0027, inferSchema\u003dTrue, header\u003dTrue)\n",
        "trips \u003d trips.drop(\u0027block_id\u0027, \u0027shape_id\u0027, \u0027wheelchair_accessible\u0027, \u0027bikes_allowed\u0027)\n",
        "trips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[stop_id: bigint, stop_code: bigint, stop_name: string, stop_lat: double, stop_lon: double]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stops \u003d spark.read.csv(\u0027../Datasets/ba_points/stops.txt\u0027, inferSchema\u003dTrue, header\u003dTrue)\n",
        "stops.drop(\u0027stop_timezone\u0027, \u0027wheelchair_boarding\u0027, \u0027stop_desc\u0027, \u0027zone_id\u0027, \u0027stop_url\u0027, \n",
        "           \u0027location_type\u0027, \u0027parent_station\u0027)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "from math import sin, cos, atan, sqrt, pi\n",
        "\n",
        "earthRadius \u003d 6.371e3\n",
        "\n",
        "def toRadians(series):\n",
        "    return series.mul(pi).div(180.0)\n",
        "\n",
        "def cosS(series):\n",
        "    return series.apply(cos)\n",
        "\n",
        "def sinS(series):\n",
        "    return series.apply(sin)\n",
        "\n",
        "def absS(series):\n",
        "    return series.abs()\n",
        "\n",
        "def sqrtS(series):\n",
        "    return series.apply(sqrt)\n",
        "\n",
        "def atan2S(series1, series2):\n",
        "    return (series1/series2).apply(atan)\n",
        "\n",
        "def geodesicdistance(point1Lat, point1Lng, point2Lat, point2Lng):\n",
        "    # Geodesic distance between two points on the Earth \n",
        "    # computed using Vincenty inverse problem formula \n",
        "    lat1, lng1 \u003d toRadians(point1Lat), toRadians(point1Lng)\n",
        "    lat2, lng2 \u003d toRadians(point2Lat), toRadians(point2Lng)\n",
        "    a \u003d cosS(lat2)*sinS(absS(lng2 - lng1))\n",
        "    b \u003d cosS(lat1)*sinS(lat2)-sinS(lat1)*cosS(lat2)*cosS(abs(lng2 - lng1))\n",
        "    c \u003d sinS(lat1)*sinS(lat2)+cosS(lat1)*cosS(lat2)*cosS(abs(lng2 - lng1))\n",
        "    return earthRadius*atan2S(sqrtS(a*a+b*b),c)*1000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "@pandas_udf(\u0027float\u0027, PandasUDFType.SCALAR)\n",
        "def distance(lat1,lon1,lat2,lon2):\n",
        "    return geodesicdistance(lat1, lon1, lat2, lon2)\n",
        "\n",
        "@pandas_udf(\u0027int\u0027, PandasUDFType.GROUPED_AGG)\n",
        "def time_spread(timestamp):\n",
        "    return timestamp.max() - timestamp.min()\n",
        "\n",
        "@udf(\u0027string\u0027)\n",
        "def geohash(lat, lon):\n",
        "    return pgh.encode(lat, lon, precision\u003d7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "stops \u003d stops.withColumn(\u0027geohash\u0027, geohash(stops.stop_lat, stops.stop_lon))\n",
        "points \u003d points.withColumn(\u0027geohash\u0027, geohash(points.latitude, points.longitude))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[trip_id: string, geohash: string, stop_id: bigint, stop_sequence: int, stop_lat: double, stop_lon: double, timestamp: bigint, vehicle_id: string, start_time: string, latitude: double, longitude: double, speed: double]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stops_with_coors \u003d stop_trips.join(stops, \u0027stop_id\u0027)\n",
        "stops_with_points \u003d stops_with_coors.join(points, [\u0027trip_id\u0027, \u0027geohash\u0027])\n",
        "stops_with_points \u003d stops_with_points.drop(\u0027start_date\u0027, \u0027vehicle_label\u0027, \u0027route_id\u0027, \u0027timepoint\u0027,\n",
        "                       \u0027stop_name\u0027, \u0027stop_code\u0027, \u0027last_stop_id\u0027)\n",
        "stops_with_points.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
        "stops_with_points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Tiempo en parada por parada por colectivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, stop_id: bigint, tiempo: int]"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": "time_in_stop \u003d stops_with_points.drop(\u0027date\u0027) \\\n  .withColumn(\"distance\", distance(\u0027stop_lat\u0027, \u0027stop_lon\u0027,\u0027latitude\u0027, \u0027longitude\u0027)) \\\n  .orderBy(\"timestamp\") \\\n  .filter(col(\"distance\") \u003c 50) \\\n  .filter(stops_with_points.speed \u003c\u003d 5) \\\n  .groupby(\"vehicle_id\", \"stop_id\") \\\n  .agg(time_spread(col(\"timestamp\")).alias(\u0027tiempo\u0027)) \\\n  .withColumn(\u0027tiempo\u0027, col(\u0027tiempo\u0027) + lit(30))\ntime_in_stop.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\ntime_in_stop"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Tiempo total en parada total por colectivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, total_time_in_stop: bigint]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_time_in_stop \u003d time_in_stop.groupBy(\u0027vehicle_id\u0027).agg(F.sum(\u0027tiempo\u0027).alias(\u0027total_time_in_stop\u0027))\n",
        "total_time_in_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Tiempo total trabajado por colectivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, total_work_time: int]"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_working_time \u003d points.groupBy(\u0027vehicle_id\u0027).agg(time_spread(col(\u0027timestamp\u0027)).alias(\u0027total_work_time\u0027))\n",
        "total_working_time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Porcentaje de tiempo en parada por colectivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, total_time_in_stop: bigint, total_work_time: int, percentage_in_stop: double]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "time_distribution \u003d total_time_in_stop.join(total_working_time, \u0027vehicle_id\u0027) \\\n",
        "                        .withColumn(\u0027percentage_in_stop\u0027, (F.col(\u0027total_time_in_stop\u0027) / F.col(\u0027total_work_time\u0027))) \\\n",
        "                        .orderBy(\u0027vehicle_id\u0027)\n",
        "time_distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Paradas efectivamente paradas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "scrolled": true,
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[trip_id: string, geohash: string, stop_id: bigint, stop_sequence: int, stop_lat: double, stop_lon: double, timestamp: bigint, vehicle_id: string, start_time: string, latitude: double, longitude: double, speed: double, distance: float]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "stopped_stops \u003d stops_with_points.drop(\u0027date\u0027) \\\n",
        "  .withColumn(\"distance\", distance(\u0027stop_lat\u0027, \u0027stop_lon\u0027,\u0027latitude\u0027, \u0027longitude\u0027)) \\\n",
        "  .orderBy(\"timestamp\") \\\n",
        "  .filter(col(\"distance\") \u003c 50) \\\n",
        "  .filter(stops_with_points.speed \u003c\u003d 1) \\\n",
        "  .dropDuplicates() \\\n",
        "  .orderBy(\u0027stop_id\u0027)\n",
        "stopped_stops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, stop_sequence: int, stop_id: bigint, trip_id: string, from_timestamp: bigint, to_timestamp: bigint]"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "window \u003d Window.partitionBy(\u0027vehicle_id\u0027).orderBy(\u0027stop_sequence\u0027)\n",
        "stop_time_interval \u003d  stopped_stops \\\n",
        "    .drop(\u0027stop_lat\u0027, \u0027stop_lon\u0027, \u0027start_time\u0027, \u0027latitude\u0027, \u0027longitude\u0027, \u0027speed\u0027, \u0027distance\u0027) \\\n",
        "    .withColumn(\u0027to_timestamp\u0027, F.lead(col(\u0027timestamp\u0027)).over(window)) \\\n",
        "    .groupBy(\u0027vehicle_id\u0027, \u0027stop_sequence\u0027, \u0027stop_id\u0027, \u0027trip_id\u0027) \\\n",
        "    .agg(F.min(\u0027timestamp\u0027).alias(\u0027from_timestamp\u0027),F.max(\u0027to_timestamp\u0027).alias(\u0027to_timestamp\u0027))\n",
        "stop_time_interval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[amount: double, ticket_type: string, timestamp: bigint, vehicle_id: string]"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tickets \u003d spark.read.json(\u0027../Datasets/passengers_in_stop/sube_transactions.json\u0027, multiLine\u003dTrue)\n",
        "tickets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, stop_sequence: int, stop_id: bigint, trip_id: string, from_timestamp: bigint, to_timestamp: bigint, amount: double, ticket_type: string, timestamp: bigint]"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "passengers_per_stop \u003d stop_time_interval.join(tickets, \u0027vehicle_id\u0027) \\\n",
        "    .withColumn(\"to_timestamp\",\n",
        "                F.when(col(\"to_timestamp\").isNull(), col(\"from_timestamp\") + lit(50)).otherwise(col(\"to_timestamp\"))\n",
        "               ) \\\n",
        "    .filter(col(\"timestamp\").between(col(\"from_timestamp\"), col(\"to_timestamp\")))\n",
        "passengers_per_stop.persist(pyspark.StorageLevel.MEMORY_AND_DISK)\n",
        "passengers_per_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Cantidad total de revenue por interno"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, stop_id: bigint, from_timestamp: bigint, total_revenue: double]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_revenue_by_bus \u003d passengers_per_stop \\\n",
        "    .groupBy(\u0027vehicle_id\u0027, \u0027stop_id\u0027, \u0027from_timestamp\u0027) \\\n",
        "    .agg(F.sum(\u0027amount\u0027).alias(\u0027total_revenue\u0027))\n",
        "total_revenue_by_bus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Tipo de ticket por parada por colectivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, stop_id: bigint, ticket_type: string, from_timestamp: bigint, amount: bigint]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ticket_amount_by_stop_by_bus \u003d passengers_per_stop \\\n",
        "    .groupBy(\u0027vehicle_id\u0027, \u0027stop_id\u0027, \u0027ticket_type\u0027, \u0027from_timestamp\u0027) \\\n",
        "    .agg(F.count(F.lit(1)).alias(\"amount\"))\n",
        "ticket_amount_by_stop_by_bus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Cantidad de pasajeros nuevos por parada por colectivo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DataFrame[vehicle_id: string, stop_id: bigint, from_timestamp: bigint, amount: bigint]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "new_passengers_per_stop \u003d passengers_per_stop \\\n",
        "    .groupBy(\u0027vehicle_id\u0027, \u0027stop_id\u0027, \u0027from_timestamp\u0027) \\\n",
        "    .agg(F.count(F.lit(3)).alias(\"amount\"))\n",
        "new_passengers_per_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### Escribir al PostgreSQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [],
      "source": [
        "time_in_stop.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \"public.time_in_stop_by_bus_by_stop\",\n",
        "                        properties\u003d{\"user\": \"sa\", \"password\": \"password\"})\n",
        "total_time_in_stop.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \"public.time_in_stop_by_bus\",\n",
        "                              properties\u003d{\"user\": \"sa\", \"password\": \"password\"})\n",
        "total_working_time.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \"public.total_worked_time_by_bus\",\n",
        "                              properties\u003d{\"user\": \"sa\", \"password\": \"password\"})\n",
        "time_distribution.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \"public.stop_time_efficiency\",\n",
        "                             properties\u003d{\"user\": \"sa\", \"password\": \"password\"})\n",
        "stopped_stops.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \"public.stopped_stops\",\n",
        "                         properties\u003d{\"user\": \"sa\", \"password\": \"password\"})\n",
        "ticket_amount_by_stop_by_bus.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \n",
        "                                        \"public.ticket_amount_by_stop_by_bus\",\n",
        "                                        properties\u003d{\"user\": \"sa\", \"password\": \"password\"})\n",
        "new_passengers_per_stop.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \"public.new_passengers_per_stop\",\n",
        "                             properties\u003d{\"user\": \"sa\", \"password\": \"password\"})\n",
        "total_revenue_by_bus.write.jdbc(\"jdbc:postgresql://postgres:5432/sparkdb\", \"public.total_revenue_by_bus\",\n",
        "                          properties\u003d{\"user\": \"sa\", \"password\": \"password\"})"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}